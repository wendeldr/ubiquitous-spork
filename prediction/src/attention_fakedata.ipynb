{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 100000\n",
      "    Positive: 8333 (8.33% of total)\n",
      "\n",
      "data shape (+1 for soz)\n",
      "(100000, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_454929/3775563775.py:54: DeprecationWarning: Non-integer input passed to bincount. In a future version of NumPy, this will be an error. (Deprecated NumPy 2.1)\n",
      "  neg, pos = np.bincount(raw_df['soz'])\n"
     ]
    }
   ],
   "source": [
    "def attention_fakedata():\n",
    "    rng = np.random.default_rng(42)\n",
    "    ratio = 11.0\n",
    "\n",
    "    # Generate synthetic data with \n",
    "    n_samples = 100000\n",
    "    n_pos = int(n_samples / (ratio + 1))  # Calculate number of positive samples\n",
    "    n_neg = n_samples - n_pos  # Remaining are negative samples\n",
    "\n",
    "    # Create labels array with the correct ratio\n",
    "    labels = np.concatenate([np.zeros(n_neg), np.ones(n_pos)])\n",
    "\n",
    "    # Shuffle the labels\n",
    "    rng.shuffle(labels)\n",
    "\n",
    "    # Create a DataFrame with the synthetic data\n",
    "    raw_df = pd.DataFrame({'soz': labels})\n",
    "\n",
    "    # Using boolean indexing:\n",
    "    mask0 = raw_df['soz'] == 0\n",
    "    mask1 = raw_df['soz'] == 1\n",
    "\n",
    "    raw_df.loc[mask0, 'f1'] = rng.normal(6, 0.3, size=mask0.sum())\n",
    "    raw_df.loc[mask1, 'f1'] = rng.gamma(1, .11, size=mask1.sum())\n",
    "\n",
    "\n",
    "    raw_df['f2'] = rng.normal(10, 5, size=n_samples)\n",
    "    # raw_df['f3'] = rng.poisson(lam=3, size=n_samples)\n",
    "\n",
    "    # # plot features on one 1,3 plot grouped by soz\n",
    "    # fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # # Plot feature 1 distributions\n",
    "    # sns.histplot(data=raw_df, hue='soz', x='f1', ax=axes[0])\n",
    "    # axes[0].set_title('Feature 1 Distribution by SOZ')\n",
    "    # axes[0].set_xlabel('SOZ')\n",
    "    # axes[0].set_ylabel('Value')\n",
    "\n",
    "    # # Plot feature 2 distributions  \n",
    "    # sns.histplot(data=raw_df, hue='soz', x='f2', ax=axes[1])\n",
    "    # axes[1].set_title('Feature 2 Distribution by SOZ')\n",
    "    # axes[1].set_xlabel('SOZ')\n",
    "    # axes[1].set_ylabel('Value')\n",
    "\n",
    "    # # Plot feature 3 distributions\n",
    "    # sns.histplot(data=raw_df, hue='soz', x='f3', ax=axes[2])\n",
    "    # axes[2].set_title('Feature 3 Distribution by SOZ')\n",
    "    # axes[2].set_xlabel('SOZ')\n",
    "    # axes[2].set_ylabel('Value')\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    neg, pos = np.bincount(raw_df['soz'])\n",
    "    total = neg + pos\n",
    "    print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "        total, pos, 100 * pos / total))\n",
    "\n",
    "    initial_bias = np.log([pos/neg])\n",
    "\n",
    "    print('data shape (+1 for soz)')\n",
    "    print(raw_df.shape)\n",
    "    return raw_df,initial_bias\n",
    "\n",
    "raw_df, initial_bias = attention_fakedata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a utility from sklearn to split and shuffle your dataset.\n",
    "train_df, test_df = train_test_split(raw_df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('soz')).reshape(-1, 1)\n",
    "bool_train_labels = train_labels[:, 0] != 0\n",
    "val_labels = np.array(val_df.pop('soz')).reshape(-1, 1)\n",
    "test_labels = np.array(test_df.pop('soz')).reshape(-1, 1)\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "# train_features = np.clip(train_features, -5, 5)\n",
    "# val_features = np.clip(val_features, -5, 5)\n",
    "# test_features = np.clip(test_features, -5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Self-attention model mimicking your Keras architecture.\n",
    "class SelfAttentionModel(nn.Module):\n",
    "    def __init__(self, N_value):\n",
    "        super(SelfAttentionModel, self).__init__()\n",
    "        self.N_value = N_value\n",
    "        # Project each feature (token) from 1 -> 3 to mimic key_dim=3.\n",
    "        self.proj = nn.Linear(1, 3)\n",
    "        # MultiheadAttention: embed_dim=3, one head, batch_first=True.\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=3, num_heads=1, batch_first=True)\n",
    "        # Layer normalization after flattening the attention output.\n",
    "        self.ln = nn.LayerNorm(N_value * 3)\n",
    "        # MLP block: first dense layer with 4 * N_value units.\n",
    "        self.fc1 = nn.Linear(N_value * 3, 4 * N_value)\n",
    "        # Output layer with one unit.\n",
    "        self.fc2 = nn.Linear(4 * N_value, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, N_value)\n",
    "        # Reshape to (batch, N_value, 1) so each feature is a token.\n",
    "        x = x.unsqueeze(-1)\n",
    "        # Project tokens to dimension 3.\n",
    "        x = self.proj(x)\n",
    "        # Self-attention (queries, keys, and values are the same).\n",
    "        attn_output, attn_weights = self.attn(x, x, x)\n",
    "        # Flatten the attention output to (batch, N_value * 3).\n",
    "        x = attn_output.view(attn_output.size(0), -1)\n",
    "        # Apply layer normalization.\n",
    "        x = self.ln(x)\n",
    "        # MLP block with ReLU activation.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # Sigmoid activation to get probabilities.\n",
    "        x = torch.sigmoid(x)\n",
    "        return x, attn_weights\n",
    "\n",
    "# Early stopping callback implementation.\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_state = None\n",
    "    \n",
    "    def __call__(self, metric, model):\n",
    "        score = metric\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_state = model.state_dict()\n",
    "        elif (self.mode == 'max' and score < self.best_score) or (self.mode == 'min' and score > self.best_score):\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "# Training loop using torchmetrics.\n",
    "def train_model(model, train_loader, val_loader, epochs=200, lr=1e-3, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()  # Binary crossentropy loss\n",
    "\n",
    "    # Initialize torchmetrics for training.\n",
    "    train_loss_metric = MeanMetric()\n",
    "    train_tp = BinaryTruePositives()\n",
    "    train_fp = BinaryFalsePositives()\n",
    "    train_tn = BinaryTrueNegatives()\n",
    "    train_fn = BinaryFalseNegatives()\n",
    "    train_accuracy = BinaryAccuracy()\n",
    "    train_precision = BinaryPrecision()\n",
    "    train_recall = BinaryRecall()\n",
    "    train_auc = BinaryAUROC()\n",
    "    train_prc = BinaryAUCPR()\n",
    "\n",
    "    # Initialize torchmetrics for validation.\n",
    "    val_loss_metric = MeanMetric()\n",
    "    val_tp = BinaryTruePositives()\n",
    "    val_fp = BinaryFalsePositives()\n",
    "    val_tn = BinaryTrueNegatives()\n",
    "    val_fn = BinaryFalseNegatives()\n",
    "    val_accuracy = BinaryAccuracy()\n",
    "    val_precision = BinaryPrecision()\n",
    "    val_recall = BinaryRecall()\n",
    "    val_auc = BinaryAUROC()\n",
    "    val_prc = BinaryAUCPR()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True, mode='max')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Reset metrics at the start of each epoch.\n",
    "        model.train()\n",
    "        train_loss_metric.reset()\n",
    "        train_brier.reset()\n",
    "        train_tp.reset()\n",
    "        train_fp.reset()\n",
    "        train_tn.reset()\n",
    "        train_fn.reset()\n",
    "        train_accuracy.reset()\n",
    "        train_precision.reset()\n",
    "        train_recall.reset()\n",
    "        train_auc.reset()\n",
    "        train_prc.reset()\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(features)\n",
    "            # outputs shape: (batch, 1) – squeeze to (batch,)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update training metrics.\n",
    "            train_loss_metric.update(loss.item())\n",
    "            # For classification metrics, convert labels to int (0 or 1)\n",
    "            train_tp.update(outputs, labels.int())\n",
    "            train_fp.update(outputs, labels.int())\n",
    "            train_tn.update(outputs, labels.int())\n",
    "            train_fn.update(outputs, labels.int())\n",
    "            train_accuracy.update(outputs, labels.int())\n",
    "            train_precision.update(outputs, labels.int())\n",
    "            train_recall.update(outputs, labels.int())\n",
    "            train_auc.update(outputs, labels.int())\n",
    "            train_prc.update(outputs, labels.int())\n",
    "        \n",
    "        # Compute average training metrics.\n",
    "        avg_train_loss = train_loss_metric.compute()\n",
    "        avg_train_acc = train_accuracy.compute()\n",
    "        avg_train_precision = train_precision.compute()\n",
    "        avg_train_recall = train_recall.compute()\n",
    "        avg_train_auc = train_auc.compute()\n",
    "        avg_train_prc = train_prc.compute()\n",
    "\n",
    "        # Evaluate on the validation set.\n",
    "        model.eval()\n",
    "        val_loss_metric.reset()\n",
    "        val_tp.reset()\n",
    "        val_fp.reset()\n",
    "        val_tn.reset()\n",
    "        val_fn.reset()\n",
    "        val_accuracy.reset()\n",
    "        val_precision.reset()\n",
    "        val_recall.reset()\n",
    "        val_auc.reset()\n",
    "        val_prc.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features = features.to(device)\n",
    "                labels = labels.to(device).float()\n",
    "                outputs, _ = model(features)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss_metric.update(loss.item())\n",
    ".                val_tp.update(outputs, labels.int())\n",
    "                val_fp.update(outputs, labels.int())\n",
    "                val_tn.update(outputs, labels.int())\n",
    "                val_fn.update(outputs, labels.int())\n",
    "                val_accuracy.update(outputs, labels.int())\n",
    "                val_precision.update(outputs, labels.int())\n",
    "                val_recall.update(outputs, labels.int())\n",
    "                val_auc.update(outputs, labels.int())\n",
    "                val_prc.update(outputs, labels.int())\n",
    "        \n",
    "        avg_val_loss = val_loss_metric.compute()\n",
    "        avg_val_brier = val_brier.compute()\n",
    "        avg_val_acc = val_accuracy.compute()\n",
    "        avg_val_precision = val_precision.compute()\n",
    "        avg_val_recall = val_recall.compute()\n",
    "        avg_val_auc = val_auc.compute()\n",
    "        avg_val_prc = val_prc.compute()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train - Loss: {avg_train_loss:.4f}, Brier: {avg_train_brier:.4f}, Acc: {avg_train_acc:.4f}, Precision: {avg_train_precision:.4f}, Recall: {avg_train_recall:.4f}, AUC: {avg_train_auc:.4f}, PRC: {avg_train_prc:.4f}\")\n",
    "        print(f\"  Val   - Loss: {avg_val_loss:.4f}, Brier: {avg_val_brier:.4f}, Acc: {avg_val_acc:.4f}, Precision: {avg_val_precision:.4f}, Recall: {avg_val_recall:.4f}, AUC: {avg_val_auc:.4f}, PRC: {avg_val_prc:.4f}\")\n",
    "        \n",
    "        # Use the validation PRC metric for early stopping.\n",
    "        early_stopping(avg_val_prc, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            model.load_state_dict(early_stopping.best_state)\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# Assume train_loader and val_loader are DataLoader objects and N_value is the number of features.\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SelfAttentionModel(N_value)\n",
    "# trained_model = train_model(model, train_loader, val_loader, epochs=200, lr=1e-3, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(4,3,figsize=(15,15))\n",
    "\n",
    "\n",
    "attn = model.layers[2]\n",
    "x = train_features.reshape(-1, train_features.shape[-1], 1)\n",
    "_, attention_scores = attn(x,x, return_attention_scores=True) # take one sample\n",
    "\n",
    "\n",
    "mean_all = attention_scores.numpy().squeeze().mean(axis=0)\n",
    "mean_0 = attention_scores.numpy()[train_labels[:,0]==0,0,:,:].squeeze().mean(axis=0)\n",
    "mean_1 = attention_scores.numpy()[train_labels[:,0]==1,0,:,:].squeeze().mean(axis=0)\n",
    "\n",
    "sns.heatmap(mean_all, annot=True, cbar=True,square=True, fmt='.2f', ax=axes[0,0], cmap='hot',vmin=0,vmax=1)\n",
    "sns.heatmap(mean_0, annot=True, cbar=True,square=True, fmt='.2f', ax=axes[0,1], cmap='hot',vmin=0,vmax=1)\n",
    "sns.heatmap(mean_1, annot=True, cbar=True,square=True, fmt='.2f', ax=axes[0,2], cmap='hot',vmin=0,vmax=1)\n",
    "axes[0,0].set_title('All')\n",
    "axes[0,1].set_title('soz=0')\n",
    "axes[0,2].set_title('soz=1')\n",
    "plt.suptitle(f'Attention Scores | {history.history[\"prc\"][-1]:.2f}')\n",
    "\n",
    "\n",
    "\n",
    "a = attention_scores.numpy().squeeze()\n",
    "zero_mask = train_labels[:,0]==0\n",
    "one_mask = train_labels[:,0]==1\n",
    "\n",
    "for i in range(a.shape[1]):\n",
    "    for j in range(a.shape[2]):\n",
    "        zeros = a[zero_mask,i,j]\n",
    "        ones = a[one_mask,i,j]\n",
    "\n",
    "        sns.histplot(zeros,bins=20,label=\"zeros\", alpha=0.5, color=\"blue\",ax=axes[i+1,j],kde=True)\n",
    "        sns.histplot(ones,bins=20,label=\"ones\", alpha=0.5, color=\"red\",ax=axes[i+1,j],kde=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {'weights':model.layers[2].get_weights()[0],\n",
    "         'biases':model.layers[2].get_weights()[1]}\n",
    "keys = {'weights':model.layers[2].get_weights()[2],\n",
    "        'biases':model.layers[2].get_weights()[3]}\n",
    "values = {'weights':model.layers[2].get_weights()[4],\n",
    "          'biases':model.layers[2].get_weights()[5]}\n",
    "projection = {'weights':model.layers[2].get_weights()[6],\n",
    "              'biases':model.layers[2].get_weights()[7]}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection['weights'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values['weights'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection['biases'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [np.column_stack((np.linspace(-1,1, 100), k*np.ones(100)/10.)) for k in range(-10,11)] +\\\n",
    "            [np.column_stack((k*np.ones(100)/10.,np.linspace(-1,1, 100))) for k in range(-10,11) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query['biases'][0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['f1','f2']].values[0].reshape(1,-1) * query['weights'][0,:,:].T + query['biases'][0,:].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw 2d vector\n",
    "%matplotlib widget\n",
    "\n",
    "idx = 1\n",
    "a = train_df[['f1','f2']].values[idx]\n",
    "print(a)\n",
    "plt.plot([0,a[0]], [0,a[1]])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "a_hat = train_df[['f1','f2']].values[idx] * query['weights'][0,:,:].T + query['biases'][0,:].reshape(-1,1)\n",
    "print(a_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# Define necessary lines to plot a grid-- this will represent the vanilla \"input space\".\n",
    "grids = [np.column_stack((np.linspace(-1,1, 100), k*np.ones(100)/10.)) for k in range(-10,11)] +\\\n",
    "        [np.column_stack((k*np.ones(100)/10.,np.linspace(-1,1, 100))) for k in range(-10,11) ]\n",
    "\n",
    "\n",
    "grid_lines = []\n",
    "\n",
    "for grid in grids:\n",
    "    vals = np.array(grid)\n",
    "    l, = ax.plot(vals[:,0],vals[:,1], color='grey', alpha=.5)\n",
    "    grid_lines.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "transformed_lines = []\n",
    "for k in range(len(grid_lines)):\n",
    "    ln = grid_lines[k]\n",
    "    grid = grids[k]\n",
    "    # vals = grid @ query['weights'][0,:,:].T + query['biases'][0,:]\n",
    "    break\n",
    "    l, = ax.plot(vals[:,0],vals[:,1], color='grey', alpha=.5)\n",
    "    transformed_lines.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query['weights'][0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid.shape  @ query['weights'] + query['biases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grid in grids:\n",
    "    vals = np.array(grid)\n",
    "        # Reshape grid_line to (100, 2, 1)\n",
    "    grid_line_reshaped = vals.reshape(100, 2, 1)\n",
    "\n",
    "    # Squeeze the extra dimension from the weight: (1,1,3) -> (1,3)\n",
    "    query_weight_squeezed = np.squeeze(query['weights'], axis=0)\n",
    "\n",
    "    # Apply the query transformation:\n",
    "    transformed_query = grid_line_reshaped @ query_weight_squeezed + query['biases']\n",
    "\n",
    "    l, = ax.plot(vals[:,0],vals[:,1], color='grey', alpha=.5)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def full_attention_transform(X, query, keys, values, projection):\n",
    "    \"\"\"\n",
    "    Applies the full self-attention transformation.\n",
    "    \n",
    "    Parameters:\n",
    "      X: np.array of shape (1, 2) representing one example with 2 tokens.\n",
    "      query: dict with 'weights' (shape (1,1,3)) and 'biases' (shape (1,3))\n",
    "      keys: dict with 'weights' (shape (1,1,3)) and 'biases' (shape (1,3))\n",
    "      values: dict with 'weights' (shape (1,1,3)) and 'biases' (shape (1,3))\n",
    "      projection: dict with 'weights' (shape (1,3,1)) and 'biases' (shape (1,))\n",
    "    \n",
    "    Returns:\n",
    "      output: the final transformed output after projection, shape (1,2)\n",
    "      (Also returns intermediate Q, K, V, attention weights, and attention output if needed)\n",
    "    \"\"\"\n",
    "    # Reshape input X to (batch, tokens, feature_dim) = (1,2,1)\n",
    "    X_reshaped = X.reshape(1, -1, 1)  # Now shape (1,2,1)\n",
    "    \n",
    "    # Squeeze out the extra leading dimensions for weights:\n",
    "    # Each weight maps from 1 -> 3 so final weight shape is (1,3) and bias is (3,)\n",
    "    W_q = np.squeeze(query['weights'], axis=0)   # (1,3)\n",
    "    b_q = np.squeeze(query['biases'], axis=0)      # (3,)\n",
    "    W_k = np.squeeze(keys['weights'], axis=0)      # (1,3)\n",
    "    b_k = np.squeeze(keys['biases'], axis=0)         # (3,)\n",
    "    W_v = np.squeeze(values['weights'], axis=0)    # (1,3)\n",
    "    b_v = np.squeeze(values['biases'], axis=0)       # (3,)\n",
    "    \n",
    "    # For projection, remove the extra leading dimension.\n",
    "    W_p = np.squeeze(projection['weights'], axis=0)  # (3,1)\n",
    "    b_p = np.squeeze(projection['biases'], axis=0)     # (1,) or scalar\n",
    "    \n",
    "    # Compute Q, K, V for each token. Each is computed as: token * weight + bias.\n",
    "    # X_reshaped is (1,2,1), and np.matmul will perform the multiplication on the last axis.\n",
    "    Q = np.matmul(X_reshaped, W_q) + b_q  # Resulting shape: (1,2,3)\n",
    "    K = np.matmul(X_reshaped, W_k) + b_k  # (1,2,3)\n",
    "    V = np.matmul(X_reshaped, W_v) + b_v  # (1,2,3)\n",
    "    \n",
    "    # Compute scaled dot-product attention.\n",
    "    # For each example in the batch, compute scores = Q @ K^T.\n",
    "    # Q: (1,2,3) and K^T: (1,3,2) gives scores of shape (1,2,2).\n",
    "    d_k = Q.shape[-1]  # Should be 3.\n",
    "    scores = np.matmul(Q, np.transpose(K, (0, 2, 1))) / np.sqrt(d_k)\n",
    "    \n",
    "    # Define softmax function (applied along the last axis)\n",
    "    def softmax(x, axis=-1):\n",
    "        exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "    \n",
    "    attn_weights = softmax(scores, axis=-1)  # (1,2,2)\n",
    "    \n",
    "    # Compute attention output: weighted sum of V by attention weights.\n",
    "    # attn_weights: (1,2,2), V: (1,2,3) -> output shape (1,2,3)\n",
    "    attention_output = np.matmul(attn_weights, V)\n",
    "    \n",
    "    # Apply the projection transformation for each token:\n",
    "    # Multiply each token vector (3-dim) by W_p (3,1) and add bias.\n",
    "    # This gives a shape (1,2,1).\n",
    "    proj_output = np.matmul(attention_output, W_p) + b_p\n",
    "    \n",
    "    # Optionally, flatten the output to shape (1,2)\n",
    "    final_output = proj_output.reshape(1, -1)\n",
    "    \n",
    "    return final_output, Q, K, V, attn_weights, attention_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each weight maps from 1 -> 3 so final weight shape is (1,3) and bias is (3,)\n",
    "W_q = np.squeeze(query['weights'], axis=0)   # (1,3)\n",
    "b_q = np.squeeze(query['biases'], axis=0)      # (3,)\n",
    "W_k = np.squeeze(keys['weights'], axis=0)      # (1,3)\n",
    "b_k = np.squeeze(keys['biases'], axis=0)         # (3,)\n",
    "W_v = np.squeeze(values['weights'], axis=0)    # (1,3)\n",
    "b_v = np.squeeze(values['biases'], axis=0)       # (3,)\n",
    "\n",
    "# For projection, remove the extra leading dimension.\n",
    "W_p = np.squeeze(projection['weights'], axis=0)  # (3,1)\n",
    "b_p = np.squeeze(projection['biases'], axis=0)     # (1,) or scalar\n",
    "\n",
    "x = train_features.reshape(-1, train_features.shape[-1], 1)\n",
    "Q = np.matmul(x, W_q) + b_q  # Resulting shape: (1,2,3)\n",
    "K = np.matmul(x, W_k) + b_k  # (1,2,3)\n",
    "V = np.matmul(x, W_v) + b_v  # (1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = Q.reshape(-1, Q.shape[1] *Q.shape[2])\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = model.layers[2]\n",
    "x = train_features.reshape(-1, train_features.shape[-1], 1)\n",
    "out_vec, attention_scores = attn(x,x, return_attention_scores=True) # take one sample\n",
    "\n",
    "\n",
    "\n",
    "# X is (1,2) input\n",
    "\n",
    "X = x[0,:,:].reshape(1,-1)\n",
    "\n",
    "output, Q, K, V, attn_weights, attention_output = full_attention_transform(X, query, keys, values, projection)\n",
    "# print(\"Final output shape:\", output.shape)\n",
    "# print(\"Final output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "\n",
    "# D = d_model, F = dictionary_size\n",
    "# e.g. if d_model = 12288 and dictionary_size = 49152\n",
    "# then model_activations_D.shape = (12288,)\n",
    "# encoder_DF.weight.shape = (12288, 49152)\n",
    "\n",
    "class SparseAutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A one-layer autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, activation_dim: int, dict_size: int):\n",
    "        super().__init__()\n",
    "        self.activation_dim = activation_dim\n",
    "        self.dict_size = dict_size\n",
    "\n",
    "        self.encoder_DF = nn.Linear(activation_dim, dict_size, bias=True)\n",
    "        self.decoder_FD = nn.Linear(dict_size, activation_dim, bias=True)\n",
    "\n",
    "    def encode(self, model_activations_D: torch.Tensor) -> torch.Tensor:\n",
    "        return nn.ReLU()(self.encoder_DF(model_activations_D))\n",
    "    \n",
    "    def decode(self, encoded_representation_F: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder_FD(encoded_representation_F)\n",
    "    \n",
    "    def forward_pass(self, model_activations_D: torch.Tensor):\n",
    "        encoded_representation_F = self.encode(model_activations_D)\n",
    "        reconstructed_model_activations_D = self.decode(encoded_representation_F)\n",
    "        return reconstructed_model_activations_D, encoded_representation_F\n",
    "    \n",
    "\n",
    "# B = batch size, D = d_model, F = dictionary_size\n",
    "def calculate_loss(autoencoder: SparseAutoEncoder, model_activations_BD: torch.Tensor, l1_coefficient: float) -> torch.Tensor:\n",
    "    reconstructed_model_activations_BD, encoded_representation_BF = autoencoder.forward_pass(model_activations_BD)\n",
    "    reconstruction_error_BD = (reconstructed_model_activations_BD - model_activations_BD).pow(2)\n",
    "    reconstruction_error_B = einops.reduce(reconstruction_error_BD, 'B D -> B', 'sum')\n",
    "    l2_loss = reconstruction_error_B.mean()\n",
    "\n",
    "    l1_loss = l1_coefficient * encoded_representation_BF.sum()\n",
    "    loss = l2_loss + l1_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "activation_dim = Z.shape[1]\n",
    "dict_size = Z.shape[0]\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "l1_coefficient = 1e-5\n",
    "\n",
    "# Instantiate the model\n",
    "autoencoder = SparseAutoEncoder(activation_dim, dict_size)\n",
    "\n",
    "# Assuming Z is already defined and is a torch.Tensor of shape (64000, 6)\n",
    "# For example, if Z is a numpy array, you can convert it as follows:\n",
    "# Z = torch.tensor(Z, dtype=torch.float32)\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = torch.utils.data.TensorDataset(Z)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for (batch_data,) in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = calculate_loss(autoencoder, batch_data, l1_coefficient)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * batch_data.size(0)\n",
    "    epoch_loss /= len(dataset)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
